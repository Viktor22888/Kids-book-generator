import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from diffusers import StableDiffusionPipeline
import os
from PIL import Image
import base64
import io
from datetime import datetime
import time
import random
import math


class BookGenerator:
    def __init__(self, use_models=False, neural_mode="lightweight"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.use_models = use_models
        self.neural_mode = neural_mode
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")

        self.model = None
        self.tokenizer = None
        self.image_pipe = None
        self.neural_generator = None

        if use_models:
            self.model_name = "ai-forever/rugpt3small_based_on_gpt2"
            print("–ó–∞–≥—Ä—É–∑–∫–∞ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")
            self.model, self.tokenizer = self._load_text_model_with_retry()
            if not self.model:
                print("‚ö†Ô∏è  –Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–ª—É—á—à–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –±–µ–∑ –º–æ–¥–µ–ª–µ–π.")
            else:
                print("‚úÖ –Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

            print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
            self.image_pipe = self._load_image_model_with_retry()
            if not self.image_pipe:
                print("‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–ª—É—á—à–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.")
            else:
                print("‚úÖ –ú–æ–¥–µ–ª—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        else:
            if neural_mode in ["lightweight", "api", "local"]:
                try:
                    import sys
                    sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
                    from neural_generator import NeuralTextGenerator
                    print(f"ü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∞–ª—å–Ω–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ (—Ä–µ–∂–∏–º: {neural_mode})...")
                    self.neural_generator = NeuralTextGenerator(mode=neural_mode)

                    model_loaded = (
                        self.neural_generator.model is not None
                        and self.neural_generator.tokenizer is not None
                    )
                    api_available = (
                        neural_mode == "api"
                        and self.neural_generator.api_key is not None
                    )
                    if model_loaded or api_available:
                        print("‚úÖ –†–µ–∞–ª—å–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
                    else:
                        print("‚ö†Ô∏è  –ù–µ–π—Ä–æ—Å–µ—Ç—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º —Å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–æ–º.")
                        print("‚úÖ –ù–æ —ç—Ç–æ –Ω–µ –ø—Ä–æ–±–ª–µ–º–∞! –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ –∏ –±–µ–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏!")
                        self.neural_generator = None
                except ImportError as e:
                    print(f"‚ö†Ô∏è  –ú–æ–¥—É–ª—å neural_generator –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}")
                    self.neural_generator = None
                except Exception as e:
                    print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç—å: {e}")
                    self.neural_generator = None
            else:
                print("üìö –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã: –£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ë–ï–ó –Ω–µ–π—Ä–æ—Å–µ—Ç–∏")
                print("üí° –í—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–æ—Å—Ç—É–ø–Ω—ã, –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è!")

    def _load_text_model_with_retry(self, max_retries=2):
        for attempt in range(max_retries + 1):
            try:
                if attempt > 0:
                    print(f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries + 1})...")
                    time.sleep(5)

                tokenizer = AutoTokenizer.from_pretrained(
                    self.model_name,
                    local_files_only=False,
                    resume_download=True,
                )
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token

                model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    local_files_only=False,
                    resume_download=True,
                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                )
                model.to(self.device)
                model.eval()
                print("‚úÖ –Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                return model, tokenizer
            except Exception as e:
                if attempt < max_retries:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {str(e)[:100]}...")
                    print("–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ 5 —Å–µ–∫—É–Ω–¥...")
                else:
                    print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ {max_retries + 1} –ø–æ–ø—ã—Ç–æ–∫")
        return None, None

    def _load_image_model_with_retry(self, max_retries=1):
        model_name = "runwayml/stable-diffusion-v1-5"
        for attempt in range(max_retries + 1):
            try:
                if attempt > 0:
                    print(f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries + 1})...")
                    time.sleep(5)

                pipe = StableDiffusionPipeline.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                    local_files_only=False,
                    resume_download=True,
                )
                pipe = pipe.to(self.device)
                print("‚úÖ –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                return pipe
            except Exception as e:
                if attempt < max_retries:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {str(e)[:100]}...")
                else:
                    print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ—Å–ª–µ {max_retries + 1} –ø–æ–ø—ã—Ç–æ–∫")
        return None

    def generate_text(self, prompt, max_length=500, num_pages=5):
        if self.neural_generator:
            return self._generate_with_neural_network(prompt, max_length, num_pages)
        if self.model and self.tokenizer:
            return self._generate_with_model(prompt, max_length, num_pages)
        return self._simple_text_generation(prompt, num_pages)

    def _generate_with_neural_network(self, prompt, max_length, num_pages):
        pages = []
        story_context = []
        for page_num in range(num_pages):
            try:
                if page_num == 0:
                    page_prompt = f"–ù–∞–ø–∏—à–∏ –Ω–∞—á–∞–ª–æ –¥–µ—Ç—Å–∫–æ–π –∏—Å—Ç–æ—Ä–∏–∏ –æ {prompt}. –ù–∞—á–Ω–∏ —Å –≤–≤–µ–¥–µ–Ω–∏—è –≥–ª–∞–≤–Ω–æ–≥–æ –≥–µ—Ä–æ—è –∏ –º–µ—Å—Ç–∞ –¥–µ–π—Å—Ç–≤–∏—è. –ü–∏—à–∏ –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º –¥–ª—è –¥–µ—Ç–µ–π. –û–¥–Ω–æ-–¥–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."
                elif page_num == num_pages - 1:
                    context_summary = '. '.join([p['text'] for p in pages[:2]])[:200]
                    page_prompt = f"–ü—Ä–æ–¥–æ–ª–∂–∏ –¥–µ—Ç—Å–∫—É—é –∏—Å—Ç–æ—Ä–∏—é –æ {prompt}. –ü—Ä–µ–¥—ã–¥—É—â–∏–µ —Å–æ–±—ã—Ç–∏—è: {context_summary}. –ù–∞–ø–∏—à–∏ –∫—Ä–∞—Å–∏–≤—É—é –∫–æ–Ω—Ü–æ–≤–∫—É –∏—Å—Ç–æ—Ä–∏–∏, –≥–¥–µ –≤—Å–µ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è —Ö–æ—Ä–æ—à–æ. –û–¥–Ω–æ-–¥–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."
                else:
                    context_summary = '. '.join([p['text'] for p in pages[-1:]])[:150]
                    page_prompt = f"–ü—Ä–æ–¥–æ–ª–∂–∏ –¥–µ—Ç—Å–∫—É—é –∏—Å—Ç–æ—Ä–∏—é –æ {prompt}. –ß—Ç–æ –±—ã–ª–æ: {context_summary}. –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –¥–∞–ª—å—à–µ? –î–æ–±–∞–≤—å –Ω–æ–≤–æ–µ —Å–æ–±—ã—Ç–∏–µ –∏–ª–∏ –ø—Ä–∏–∫–ª—é—á–µ–Ω–∏–µ. –û–¥–Ω–æ-–¥–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."

                generated_text = self.neural_generator.generate_with_neural_network(
                    page_prompt,
                    max_length=150,
                    temperature=0.9,
                )
                if generated_text and len(generated_text.strip()) > 20:
                    page_text = generated_text.strip()
                    page_text = page_text.replace('**', '').replace('*', '').replace(' ', '').strip()
                    if page_text.startswith('–ò—Å—Ç–æ—Ä–∏—è') or page_text.startswith('–ñ–∏–ª-–±—ã–ª'):
                        sentences = page_text.split('.')
                        if len(sentences) > 1:
                            page_text = '. '.join(sentences[1:]).strip()
                    if prompt.lower() in page_text.lower() and page_text.lower().startswith(prompt.lower()):
                        page_text = page_text[len(prompt):].strip()
                    if not page_text.endswith(('.', '!', '?')):
                        page_text += '.'
                    if len(page_text) > 300:
                        sentences = page_text.split('.')
                        page_text = '. '.join(sentences[:2]) + '.'
                    if page_text and page_text not in [p['text'] for p in pages]:
                        pages.append({"page_number": page_num + 1, "text": page_text})
                        story_context.append(page_text)
                    else:
                        pages.append({
                            "page_number": page_num + 1,
                            "text": self._get_unique_page_text(prompt, page_num, num_pages, story_context)
                        })
                else:
                    pages.append({
                        "page_number": page_num + 1,
                        "text": self._get_unique_page_text(prompt, page_num, num_pages, story_context)
                    })
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num + 1}: {e}")
                pages.append({
                    "page_number": page_num + 1,
                    "text": self._get_unique_page_text(prompt, page_num, num_pages, story_context)
                })
        return pages

    def _get_unique_page_text(self, prompt, page_num, total_pages, context):
        if page_num == 0:
            beginnings = [
                f"–û–¥–Ω–∞–∂–¥—ã –≤ –º–∏—Ä–µ –ú–∞–π–Ω–∫—Ä–∞—Ñ—Ç–∞ –∂–∏–ª –æ—Ç–≤–∞–∂–Ω—ã–π –≥–µ—Ä–æ–π –ø–æ –∏–º–µ–Ω–∏ –°—Ç–∏–≤. –û–Ω –ª—é–±–∏–ª –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –±–µ—Å–∫—Ä–∞–π–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ—Ä—ã –∏ —Å—Ç—Ä–æ–∏—Ç—å —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã–µ —Å–æ–æ—Ä—É–∂–µ–Ω–∏—è.",
                f"–í –¥–∞–ª–µ–∫–æ–º –±–ª–æ–∫–æ–≤–æ–º –º–∏—Ä–µ –∂–∏–ª –º–∞–ª—å—á–∏–∫ –ø–æ –∏–º–µ–Ω–∏ –°—Ç–∏–≤. –ö–∞–∂–¥—ã–π –¥–µ–Ω—å –æ–Ω –æ—Ç–ø—Ä–∞–≤–ª—è–ª—Å—è –≤ –Ω–æ–≤—ã–µ –ø—Ä–∏–∫–ª—é—á–µ–Ω–∏—è, –æ—Ç–∫—Ä—ã–≤–∞—è —Ç–∞–π–Ω—ã —ç—Ç–æ–≥–æ —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞.",
                f"–ò—Å—Ç–æ—Ä–∏—è –Ω–∞—á–∞–ª–∞—Å—å, –∫–æ–≥–¥–∞ –°—Ç–∏–≤ –ø—Ä–æ—Å–Ω—É–ª—Å—è –≤ –Ω–µ–∑–Ω–∞–∫–æ–º–æ–º –º–µ—Å—Ç–µ. –í–æ–∫—Ä—É–≥ –Ω–µ–≥–æ –ø—Ä–æ—Å—Ç–∏—Ä–∞–ª—Å—è –æ–≥—Ä–æ–º–Ω—ã–π –º–∏—Ä –ú–∞–π–Ω–∫—Ä–∞—Ñ—Ç–∞, –ø–æ–ª–Ω—ã–π –∑–∞–≥–∞–¥–æ–∫ –∏ –ø—Ä–∏–∫–ª—é—á–µ–Ω–∏–π.",
            ]
            return random.choice(beginnings)
        if page_num == total_pages - 1:
            endings = [
                f"–¢–∞–∫ –∑–∞–∫–æ–Ω—á–∏–ª–æ—Å—å —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ–µ –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–µ –°—Ç–∏–≤–∞. –û–Ω –º–Ω–æ–≥–æ–º—É –Ω–∞—É—á–∏–ª—Å—è –∏ –Ω–∞—à–µ–ª –Ω–æ–≤—ã—Ö –¥—Ä—É–∑–µ–π –≤ –º–∏—Ä–µ –ú–∞–π–Ω–∫—Ä–∞—Ñ—Ç–∞. –í—Å–µ –±—ã–ª–∏ —Å—á–∞—Å—Ç–ª–∏–≤—ã!",
                f"–°—Ç–∏–≤ –≤–µ—Ä–Ω—É–ª—Å—è –¥–æ–º–æ–π —Å –Ω–æ–≤—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏ –∏ –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏—è–º–∏. –ï–≥–æ –ø—Ä–∏–∫–ª—é—á–µ–Ω–∏–µ –≤ –ú–∞–π–Ω–∫—Ä–∞—Ñ—Ç–µ —Å—Ç–∞–ª–æ –ª–µ–≥–µ–Ω–¥–æ–π, –∫–æ—Ç–æ—Ä—É—é —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞—é—Ç –¥–æ —Å–∏—Ö –ø–æ—Ä.",
                f"–í –∫–æ–Ω—Ü–µ –∫–æ–Ω—Ü–æ–≤, –≤—Å–µ –∑–∞–∫–æ–Ω—á–∏–ª–æ—Å—å —Ö–æ—Ä–æ—à–æ. –°—Ç–∏–≤ –ø–æ–Ω—è–ª, —á—Ç–æ –¥—Ä—É–∂–±–∞ –∏ —Å–º–µ–ª–æ—Å—Ç—å - —ç—Ç–æ —Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ –≤ –ª—é–±–æ–º –ø—Ä–∏–∫–ª—é—á–µ–Ω–∏–∏.",
            ]
            return random.choice(endings)
        developments = [
            f"–°—Ç–∏–≤ –æ—Ç–ø—Ä–∞–≤–∏–ª—Å—è –≤ –≥–ª—É–±–æ–∫—É—é –ø–µ—â–µ—Ä—É, –≥–¥–µ –Ω–∞—à–µ–ª —Ä–µ–¥–∫–∏–µ –∞–ª–º–∞–∑—ã. –û–Ω –±—ã–ª –æ—á–µ–Ω—å –æ—Å—Ç–æ—Ä–æ–∂–µ–Ω, —á—Ç–æ–±—ã –Ω–µ –≤—Å—Ç—Ä–µ—Ç–∏—Ç—å –≤—Ä–∞–∂–¥–µ–±–Ω—ã—Ö –º–æ–±–æ–≤.",
            f"–ù–∞ –ø—É—Ç–∏ –°—Ç–∏–≤ –≤—Å—Ç—Ä–µ—Ç–∏–ª –¥—Ä—É–∂–µ–ª—é–±–Ω—É—é –¥–µ—Ä–µ–≤–Ω—é. –ñ–∏—Ç–µ–ª–∏ –ø–æ–º–æ–≥–ª–∏ –µ–º—É –∏ –ø–æ–¥–µ–ª–∏–ª–∏—Å—å –µ–¥–æ–π. –°—Ç–∏–≤ –±—ã–ª –±–ª–∞–≥–æ–¥–∞—Ä–µ–Ω –∑–∞ –ø–æ–º–æ—â—å.",
            f"–°—Ç–∏–≤ –ø–æ—Å—Ç—Ä–æ–∏–ª –∫—Ä–∞—Å–∏–≤—ã–π –¥–æ–º –∏–∑ –¥–µ—Ä–µ–≤–∞ –∏ –∫–∞–º–Ω—è. –û–Ω —É–∫—Ä–∞—Å–∏–ª –µ–≥–æ —Ñ–∞–∫–µ–ª–∞–º–∏ –∏ —Ü–≤–µ—Ç–∞–º–∏, —á—Ç–æ–±—ã –±—ã–ª–æ —É—é—Ç–Ω–æ –∏ —Å–≤–µ—Ç–ª–æ.",
            f"–í–Ω–µ–∑–∞–ø–Ω–æ –Ω–∞ –°—Ç–∏–≤–∞ –Ω–∞–ø–∞–ª–∏ –∑–æ–º–±–∏! –ù–æ –æ–Ω –±—ã–ª –≥–æ—Ç–æ–≤ –∏ –æ—Ç–≤–∞–∂–Ω–æ –∑–∞—â–∏—â–∞–ª—Å—è —Å–≤–æ–∏–º –º–µ—á–æ–º. –í –∫–æ–Ω—Ü–µ –∫–æ–Ω—Ü–æ–≤, –æ–Ω –ø–æ–±–µ–¥–∏–ª –≤—Å–µ—Ö –≤—Ä–∞–≥–æ–≤.",
            f"–°—Ç–∏–≤ –Ω–∞—à–µ–ª –∑–∞–±—Ä–æ—à–µ–Ω–Ω—ã–π —Ö—Ä–∞–º –≤ –¥–∂—É–Ω–≥–ª—è—Ö. –í–Ω—É—Ç—Ä–∏ –µ–≥–æ –∂–¥–∞–ª–∏ –æ–ø–∞—Å–Ω—ã–µ –ª–æ–≤—É—à–∫–∏, –Ω–æ –∏ —Ü–µ–Ω–Ω—ã–µ —Å–æ–∫—Ä–æ–≤–∏—â–∞. –û–Ω –±—ã–ª –æ—á–µ–Ω—å –æ—Å—Ç–æ—Ä–æ–∂–µ–Ω.",
        ]
        return random.choice(developments)

    def _generate_with_model(self, prompt, max_length, num_pages):
        pages = []
        full_prompt = f"–î–µ—Ç—Å–∫–∞—è –∏—Å—Ç–æ—Ä–∏—è: {prompt}. –ù–∞—á–Ω–µ–º —Ä–∞—Å—Å–∫–∞–∑:"
        for page_num in range(num_pages):
            try:
                use_chat = hasattr(self.tokenizer, 'apply_chat_template') and hasattr(self.tokenizer, 'chat_template')
                if use_chat:
                    messages = [
                        {"role": "user", "content": f"–ù–∞–ø–∏—à–∏ –¥–µ—Ç—Å–∫—É—é –∏—Å—Ç–æ—Ä–∏—é –æ: {prompt}. –ü—Ä–æ–¥–æ–ª–∂–∏ —Ä–∞—Å—Å–∫–∞–∑ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num + 1}."}
                    ]
                    inputs = self.tokenizer.apply_chat_template(
                        messages,
                        add_generation_prompt=True,
                        tokenize=True,
                        return_dict=True,
                        return_tensors="pt",
                    ).to(self.device)
                    with torch.no_grad():
                        outputs = self.model.generate(
                            **inputs,
                            max_new_tokens=max_length // num_pages,
                            temperature=0.8,
                            do_sample=True,
                            top_p=0.9,
                            repetition_penalty=1.2,
                        )
                    new_text = self.tokenizer.decode(
                        outputs[0][inputs["input_ids"].shape[-1]:],
                        skip_special_tokens=True,
                    ).strip()
                else:
                    inputs = self.tokenizer.encode(full_prompt, return_tensors="pt").to(self.device)
                    with torch.no_grad():
                        outputs = self.model.generate(
                            inputs,
                            max_length=len(inputs[0]) + max_length // num_pages,
                            num_return_sequences=1,
                            temperature=0.8,
                            do_sample=True,
                            pad_token_id=self.tokenizer.eos_token_id,
                            top_p=0.9,
                            repetition_penalty=1.2,
                        )
                    generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                    new_text = generated_text[len(full_prompt):].strip()
                sentences = new_text.split('.')
                page_text = '. '.join(sentences[:2]) + '.' if len(sentences) >= 2 else new_text[:200]
                page_text = page_text.strip()
                if not page_text:
                    page_text = f"–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ {prompt}."
                pages.append({
                    "page_number": page_num + 1,
                    "text": page_text[:300],
                })
                full_prompt = generated_text[:500]
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num + 1}: {e}")
                pages.append({
                    "page_number": page_num + 1,
                    "text": self._simple_text_generation(prompt, 1)[0]["text"],
                })
        return pages

    def _simple_text_generation(self, prompt, num_pages):
        pages = []
        beginnings = [
            f"–û–¥–Ω–∞–∂–¥—ã –≤ –≤–æ–ª—à–µ–±–Ω–æ–º –º–∏—Ä–µ –ø—Ä–æ–∏–∑–æ—à–ª–∞ —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è –æ {prompt}.",
            f"–í –¥–∞–ª–µ–∫–æ–π —Å—Ç—Ä–∞–Ω–µ –∂–∏–ª –º–∞–ª–µ–Ω—å–∫–∏–π –≥–µ—Ä–æ–π, –∫–æ—Ç–æ—Ä—ã–π –æ—á–µ–Ω—å –ª—é–±–∏–ª {prompt}.",
            f"–ò—Å—Ç–æ—Ä–∏—è –Ω–∞—á–∞–ª–∞—Å—å, –∫–æ–≥–¥–∞ –Ω–∞—à –≥–µ—Ä–æ–π –≤–ø–µ—Ä–≤—ã–µ –≤—Å—Ç—Ä–µ—Ç–∏–ª {prompt}.",
            f"–í —Å–∫–∞–∑–æ—á–Ω–æ–º –ª–µ—Å—É –∂–∏–ª–∞ –¥—Ä—É–∂–Ω–∞—è —Å–µ–º—å—è, –∫–æ—Ç–æ—Ä–∞—è –∑–Ω–∞–ª–∞ –≤—Å—ë –æ {prompt}.",
        ]
        developments = [
            f"–ì–ª–∞–≤–Ω—ã–π –≥–µ—Ä–æ–π –æ—Ç–ø—Ä–∞–≤–∏–ª—Å—è –≤ –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–µ–µ –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–µ, —á—Ç–æ–±—ã —É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ –æ {prompt}.",
            f"–í –ø—É—Ç–∏ –≥–µ—Ä–æ–π –≤—Å—Ç—Ä–µ—Ç–∏–ª –º–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å {prompt}.",
            f"–í–º–µ—Å—Ç–µ —Å –Ω–æ–≤—ã–º–∏ –¥—Ä—É–∑—å—è–º–∏ –≥–µ—Ä–æ–π —É–∑–Ω–∞–ª –º–Ω–æ–≥–æ —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤ –æ {prompt}.",
            f"–ü—Ä–∏–∫–ª—é—á–µ–Ω–∏—è –ø—Ä–æ–¥–æ–ª–∂–∞–ª–∏—Å—å, –∏ –∫–∞–∂–¥—ã–π –¥–µ–Ω—å –ø—Ä–∏–Ω–æ—Å–∏–ª –Ω–æ–≤—ã–µ –æ—Ç–∫—Ä—ã—Ç–∏—è –æ {prompt}.",
            f"–ì–µ—Ä–æ–π –ø–æ–Ω—è–ª, —á—Ç–æ {prompt} - —ç—Ç–æ –Ω–µ—á—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ–µ –∏ –≤–æ–ª—à–µ–±–Ω–æ–µ.",
            f"–í–º–µ—Å—Ç–µ –æ–Ω–∏ –ø—Ä–µ–æ–¥–æ–ª–µ–ª–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏–π, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å {prompt}.",
        ]
        endings = [
            f"–í –∫–æ–Ω—Ü–µ –∫–æ–Ω—Ü–æ–≤, –≤—Å–µ –∑–∞–∫–æ–Ω—á–∏–ª–æ—Å—å —Ö–æ—Ä–æ—à–æ. –ò—Å—Ç–æ—Ä–∏—è –æ {prompt} —Å—Ç–∞–ª–∞ –ª—é–±–∏–º–æ–π –¥–ª—è –º–Ω–æ–≥–∏—Ö –¥–µ—Ç–µ–π.",
            f"–ì–µ—Ä–æ–π –≤–µ—Ä–Ω—É–ª—Å—è –¥–æ–º–æ–π —Å –Ω–æ–≤—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏ –æ {prompt} –∏ –ø–æ–¥–µ–ª–∏–ª—Å—è –∏–º–∏ —Å –¥—Ä—É–∑—å—è–º–∏.",
            f"–¢–∞–∫ –∑–∞–∫–æ–Ω—á–∏–ª–∞—Å—å —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è –æ {prompt}, –∫–æ—Ç–æ—Ä–∞—è —É—á–∏—Ç –Ω–∞—Å –±—ã—Ç—å –¥–æ–±—Ä—ã–º–∏ –∏ –ª—é–±–æ–∑–Ω–∞—Ç–µ–ª—å–Ω—ã–º–∏.",
            f"–í—Å–µ –±—ã–ª–∏ —Å—á–∞—Å—Ç–ª–∏–≤—ã, –∏ –∏—Å—Ç–æ—Ä–∏—è –æ {prompt} —Å—Ç–∞–ª–∞ –ª–µ–≥–µ–Ω–¥–æ–π, –∫–æ—Ç–æ—Ä—É—é —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞—é—Ç –¥–æ —Å–∏—Ö –ø–æ—Ä.",
        ]
        details = [
            " –û–Ω–∏ —Å–º–µ—è–ª–∏—Å—å –∏ –∏–≥—Ä–∞–ª–∏ –≤–º–µ—Å—Ç–µ.",
            " –°–æ–ª–Ω—Ü–µ —Å–≤–µ—Ç–∏–ª–æ —è—Ä–∫–æ, –∏ –≤—Å–µ –±—ã–ª–∏ —Å—á–∞—Å—Ç–ª–∏–≤—ã.",
            " –ü—Ç–∏—Ü—ã –ø–µ–ª–∏ –∫—Ä–∞—Å–∏–≤—ã–µ –ø–µ—Å–Ω–∏.",
            " –¶–≤–µ—Ç—ã —Ä–∞—Å–ø—É—Å–∫–∞–ª–∏—Å—å –≤–æ–∫—Ä—É–≥ –Ω–∏—Ö.",
        ]

        for i in range(num_pages):
            if i == 0:
                text = random.choice(beginnings) + " –≠—Ç–æ –±—ã–ª–æ –Ω–µ–æ–±—ã—á–Ω–æ–µ –ø—Ä–∏–∫–ª—é—á–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –∑–∞–ø–æ–º–Ω–∏—Ç—Å—è –Ω–∞–≤—Å–µ–≥–¥–∞."
            elif i == num_pages - 1:
                text = random.choice(endings) + " –ò –≤—Å–µ –∂–∏–ª–∏ –¥–æ–ª–≥–æ –∏ —Å—á–∞—Å—Ç–ª–∏–≤–æ!"
            else:
                text = random.choice(developments) + random.choice(details)
            pages.append({"page_number": i + 1, "text": text})
        return pages

    def generate_image(self, text_description, page_number, query=""):
        if not self.image_pipe or not self.use_models:
            return self._create_placeholder_image(page_number, query)
        try:
            image_prompt = f"–¥–µ—Ç—Å–∫–∞—è –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏—è, —è—Ä–∫–∏–µ —Ü–≤–µ—Ç–∞, –º—É–ª—å—Ç—è—à–Ω—ã–π —Å—Ç–∏–ª—å, {text_description}, –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –¥–µ—Ç—Å–∫–∞—è –∫–Ω–∏–≥–∞"
            with torch.no_grad():
                image = self.image_pipe(
                    image_prompt,
                    num_inference_steps=30,
                    guidance_scale=7.5,
                ).images[0]
            buffered = io.BytesIO()
            image.save(buffered, format="PNG")
            img_str = base64.b64encode(buffered.getvalue()).decode()
            return f"data:image/png;base64,{img_str}"
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return self._create_placeholder_image(page_number, text_description)

    def _create_placeholder_image(self, page_number=1, query=""):
        color_schemes = [
            [(255, 220, 200), (255, 180, 150)],
            [(200, 230, 255), (150, 200, 255)],
            [(255, 240, 200), (255, 220, 150)],
            [(220, 255, 220), (180, 255, 180)],
            [(255, 200, 220), (255, 170, 190)],
            [(240, 220, 255), (220, 190, 255)],
        ]
        color1, color2 = color_schemes[(page_number - 1) % len(color_schemes)]
        width, height = 512, 512
        img = Image.new('RGB', (width, height))
        pixels = img.load()
        center_x, center_y = width // 2, height // 2
        max_dist = math.sqrt(center_x ** 2 + center_y ** 2)
        for y in range(height):
            for x in range(width):
                dist = math.sqrt((x - center_x) ** 2 + (y - center_y) ** 2)
                factor = min(dist / max_dist, 1.0)
                r = int(color1[0] * (1 - factor) + color2[0] * factor)
                g = int(color1[1] * (1 - factor) + color2[1] * factor)
                b = int(color1[2] * (1 - factor) + color2[2] * factor)
                if page_number % 2 == 0:
                    circle_dist = math.sqrt((x - width // 4) ** 2 + (y - height // 4) ** 2)
                    if 30 < circle_dist < 50:
                        r = min(255, r + 30)
                        g = min(255, g + 30)
                        b = min(255, b + 30)
                pixels[x, y] = (r, g, b)
        try:
            from PIL import ImageDraw, ImageFont
            draw = ImageDraw.Draw(img)
            try:
                font = ImageFont.truetype("arial.ttf", 40)
            except:
                try:
                    font = ImageFont.truetype("C:/Windows/Fonts/arial.ttf", 40)
                except:
                    font = ImageFont.load_default()
            draw.text((20, 20), f"–°—Ç—Ä. {page_number}", fill=(100, 100, 100, 180), font=font)
        except:
            pass
        buffered = io.BytesIO()
        img.save(buffered, format="PNG")
        img_str = base64.b64encode(buffered.getvalue()).decode()
        return f"data:image/png;base64,{img_str}"

    def generate_book(self, query):
        print(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–Ω–∏–≥–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query}")
        pages_text = self.generate_text(query, max_length=1000, num_pages=6)
        pages = []
        for page_data in pages_text:
            image = self.generate_image(f"{query}, {page_data['text'][:50]}", page_data['page_number'], query)
            pages.append({
                "page_number": page_data['page_number'],
                "text": page_data['text'],
                "image": image,
            })
        book = {
            "title": f"–ò—Å—Ç–æ—Ä–∏—è –æ {query}",
            "query": query,
            "pages": pages,
            "generated_at": datetime.now().isoformat(),
            "total_pages": len(pages),
        }
        return book